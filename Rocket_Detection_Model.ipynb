{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d81d2b53-3653-4328-96d7-ab21ccc496ce",
   "metadata": {
    "id": "f771f522"
   },
   "source": [
    "# Rocket Objects Detection ðŸš€"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86f68733-3ba6-4c1e-8e1f-3572a9bba695",
   "metadata": {},
   "source": [
    "# 1. Problem Definition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fe1cabb-577b-4a8c-9c2e-a8ec35c4fa08",
   "metadata": {},
   "source": [
    "## 1.1. Project Goal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78ef9e24-94ed-45d6-acec-f817036ccc42",
   "metadata": {},
   "source": [
    "The goal of this project is to develop a robust and efficient object detection model capable of accurately identifying and localizing various components of rockets in images and videos. This includes detecting engine flames, rocket bodies, and the tiny spec of the rocket once it is in space. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97187fbc-005a-456d-a5e4-c63c1dc826d3",
   "metadata": {},
   "source": [
    "## 1.2. Problem Statement"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8e75877-219e-4f5d-8418-87e111a52eb3",
   "metadata": {},
   "source": [
    "Develop an automated system to detect and classify rocket components (engine flames, rocket bodies, and space observation) in real-time during launches, reducing reliance on manual observation and improving accuracy and efficiency."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "540c8d41-bc30-4dc1-ac32-1f667df946dd",
   "metadata": {},
   "source": [
    "## 1.3. Impact of the Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4e928b1-a9ca-4842-9464-8657f9352a84",
   "metadata": {},
   "source": [
    "- **Enhanced Safety**: Real-time detection helps identify and address issues with the rockets promptly.\n",
    "- **Efficiency**: Saves time and resources by automating the detection process.\n",
    "- **Data Insights**: Enables better analysis and improvements in rocket design and performance.\n",
    "- **Scalability**: Can monitor multiple launches simultaneously.\n",
    "- **Innovation**: Advances computer vision and machine learning in aerospace applications."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9c4acb1-86b6-4727-aa39-8f5e67edc427",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7caccd4d-5018-43f8-89fa-78a15b22bea2",
   "metadata": {},
   "source": [
    "# 2. Data Collection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "290ba575-4c0b-4e7a-a833-07ab61eda47c",
   "metadata": {},
   "source": [
    "## 2.1. Source of Data\n",
    "The dataset is sourced from NASASpaceflight on Roboflow - Rocket Detect Computer Vision Project, which can be found at the following URL: https://universe.roboflow.com/nasaspaceflight/rocket-detect"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9264efe-d767-4b09-bfed-52ae3509b5e0",
   "metadata": {},
   "source": [
    "## 2.2. Composition of the Dataset\n",
    "\n",
    "The dataset used for this project is specifically designed to train models for detecting various components of rockets. The dataset is composed of images labeled with three distinct classes:\n",
    "\n",
    "1. **Engine Flames**: The fire produced by the rocket during launch.\n",
    "2. **Rocket Body**: The main body of the launch vehicle.\n",
    "3. **Space**: The tiny spec in the sky representing the rocket after it has ascended into space.\n",
    "\n",
    "### Key Characteristics:\n",
    "- **Total Images**: The original dataset contains 12,303 images, but for this project, only half of them (approximately 6,150 images) were used and split into train, validation and test folders.\n",
    "- **Labeling**: Each image is accompanied by a label file that includes the class and bounding box coordinates for each detected object.\n",
    "- **Negative Samples**: Some images have no labels (contain no detectable objects). These negative samples are crucial for training the model to distinguish between images with and without relevant objects, improving its overall accuracy, robustness and reducing overfitting.\n",
    "\n",
    "### Example of Labeling:\n",
    "Each label file contains lines of text, where each line represents an object in the image. The format is:\n",
    "```\n",
    "class_id x_center y_center width height\n",
    "```\n",
    "- **class_id**: The class index (0 for Engine Flames, 1 for Rocket Body, 2 for Space).\n",
    "- **x_center**: The x-coordinate of the center of the bounding box (normalized between 0 and 1).\n",
    "- **y_center**: The y-coordinate of the center of the bounding box (normalized between 0 and 1).\n",
    "- **width**: The width of the bounding box (normalized between 0 and 1).\n",
    "- **height**: The height of the bounding box (normalized between 0 and 1)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f713aa7-e31d-464d-81c9-ac5d2026604a",
   "metadata": {},
   "source": [
    "## 2.3. Data Organization\n",
    "\n",
    "The dataset is organized into images and corresponding labels, with each label file containing the class and bounding box coordinates for the objects in the image.\n",
    "Here's how the dataset is organized:\n",
    "\n",
    "```\n",
    "dataset/\n",
    "â”œâ”€â”€ images/\n",
    "â”‚   â”œâ”€â”€ train/\n",
    "â”‚   â”‚   â”œâ”€â”€ image1.jpg\n",
    "â”‚   â”‚   â”œâ”€â”€ image2.jpg\n",
    "â”‚   â”‚   â””â”€â”€ ...\n",
    "â”‚   â”œâ”€â”€ val/\n",
    "â”‚   â”‚   â”œâ”€â”€ image1.jpg\n",
    "â”‚   â”‚   â”œâ”€â”€ image2.jpg\n",
    "â”‚   â”‚   â””â”€â”€ ...\n",
    "â”‚   â””â”€â”€ test/\n",
    "â”‚       â”œâ”€â”€ image1.jpg\n",
    "â”‚       â”œâ”€â”€ image2.jpg\n",
    "â”‚       â””â”€â”€ ...\n",
    "â”œâ”€â”€ labels/\n",
    "â”‚   â”œâ”€â”€ train/\n",
    "â”‚   â”‚   â”œâ”€â”€ image1.txt\n",
    "â”‚   â”‚   â”œâ”€â”€ image2.txt\n",
    "â”‚   â”‚   â””â”€â”€ ...\n",
    "â”‚   â”œâ”€â”€ val/\n",
    "â”‚   â”‚   â”œâ”€â”€ image1.txt\n",
    "â”‚   â”‚   â”œâ”€â”€ image2.txt\n",
    "â”‚   â”‚   â””â”€â”€ ...\n",
    "â”‚   â””â”€â”€ test/\n",
    "â”‚       â”œâ”€â”€ image1.txt\n",
    "â”‚       â”œâ”€â”€ image2.txt\n",
    "â”‚       â””â”€â”€ ...\n",
    "â””â”€â”€ data.yaml\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93fd8bbe-e06e-48c1-9085-535bc8eb05bd",
   "metadata": {
    "id": "49a33697"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ba610ea-b7a3-4b86-9947-7f6a7462b8d6",
   "metadata": {
    "id": "559b1c57"
   },
   "source": [
    "# 3. Data Exploration and Visualization (EDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "jQ7CGqB5h01b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jQ7CGqB5h01b",
    "outputId": "0b15ae1b-7e51-4378-abe6-4d4e0cf2c412"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "\n",
    "import cv2\n",
    "import torch\n",
    "import shutil\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from ultralytics import YOLO\n",
    "from google.colab import drive\n",
    "\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.device_count())\n",
    "\n",
    "\n",
    "drive.mount('/content/drive', force_remount=True)\n",
    "dataset_path = '/content/drive/My Drive/yolo_dataset/data.yaml'\n",
    "model_path = '/content/drive/My Drive/yolo_dataset/last_v8m.pt'\n",
    "save_dir = '/content/drive/My Drive/yolo_dataset'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1c45a75-06a9-4cf6-861c-714fb2e42161",
   "metadata": {},
   "source": [
    "## 3.1. Visualizing the Images & Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fd55f8a-58c8-40ac-9a1a-8b22bb758df5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "images_folder = '6k_dataset/images'\n",
    "labels_folder = '6k_dataset/labels'\n",
    "\n",
    "labels = {\n",
    "    0: 'Engine Flames',\n",
    "    1: 'Rocket Body',\n",
    "    2: 'Space'\n",
    "}\n",
    "\n",
    "# Load a random sample of images and their labels\n",
    "def load_random_samples(num_samples=5):\n",
    "    image_files = [f for f in os.listdir(images_folder) if f.endswith('.jpg')]\n",
    "    random_samples = random.sample(image_files, num_samples)\n",
    "    \n",
    "    samples = []\n",
    "    for image_file in random_samples:\n",
    "        image_path = os.path.join(images_folder, image_file)\n",
    "        label_file = image_file.replace('.jpg', '.txt')\n",
    "        label_path = os.path.join(labels_folder, label_file)\n",
    "        \n",
    "        with open(label_path, 'r') as f:\n",
    "            label_data = f.readlines()\n",
    "        \n",
    "        samples.append((image_path, label_data))\n",
    "    \n",
    "    return samples\n",
    "\n",
    "# Visualize images with bounding boxes\n",
    "def visualize_samples(samples):\n",
    "    for image_path, label_data in samples:\n",
    "        image = cv2.imread(image_path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        for line in label_data:\n",
    "            parts = line.strip().split()\n",
    "            label = int(parts[0])\n",
    "            x_center = float(parts[1])\n",
    "            y_center = float(parts[2])\n",
    "            width = float(parts[3])\n",
    "            height = float(parts[4])\n",
    "            \n",
    "            img_height, img_width, _ = image.shape\n",
    "            x_center *= img_width\n",
    "            y_center *= img_height\n",
    "            width *= img_width\n",
    "            height *= img_height\n",
    "            \n",
    "            x_min = int(x_center - width / 2)\n",
    "            y_min = int(y_center - height / 2)\n",
    "            x_max = int(x_center + width / 2)\n",
    "            y_max = int(y_center + height / 2)\n",
    "            \n",
    "            cv2.rectangle(image, (x_min, y_min), (x_max, y_max), (255, 0, 0), 2)\n",
    "            cv2.putText(image, labels[label], (x_min, y_min - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (255, 0, 0), 2)\n",
    "        \n",
    "        plt.figure(figsize=(10, 10))\n",
    "        plt.imshow(image)\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "\n",
    "samples = load_random_samples(num_samples=5)\n",
    "visualize_samples(samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6323f94-7be2-4db5-9891-cc33f43cb79a",
   "metadata": {},
   "source": [
    "![sample1](https://i.imgur.com/eqvFsFc.png)\n",
    "![sample2](https://i.imgur.com/U7ljJLI.png)\n",
    "![sample3](https://i.imgur.com/Xz661o0.png)\n",
    "![sample4](https://i.imgur.com/cDCH8tX.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cbcff9c-cec3-48db-a98c-21a6e2f32c43",
   "metadata": {},
   "source": [
    "## 3.2. Visualizing the Class Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4596bdc-fa44-4661-a1e4-d5546a1a605b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Count the class distribution\n",
    "def count_class_distribution():\n",
    "    class_counts = {0: 0, 1: 0, 2: 0}\n",
    "    \n",
    "    label_files = [f for f in os.listdir(labels_folder) if f.endswith('.txt')]\n",
    "    \n",
    "    for label_file in label_files:\n",
    "        label_path = os.path.join(labels_folder, label_file)\n",
    "        \n",
    "        with open(label_path, 'r') as f:\n",
    "            label_data = f.readlines()\n",
    "        \n",
    "        for line in label_data:\n",
    "            parts = line.strip().split()\n",
    "            label = int(parts[0])\n",
    "            class_counts[label] += 1\n",
    "    \n",
    "    return class_counts\n",
    "\n",
    "def plot_class_distribution(class_counts):\n",
    "    classes = [labels[key] for key in class_counts.keys()]\n",
    "    counts = [class_counts[key] for key in class_counts.keys()]\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.bar(classes, counts, color=['blue', 'green', 'red'])\n",
    "    plt.xlabel('Class')\n",
    "    plt.ylabel('Count')\n",
    "    plt.title('Class Distribution in Dataset')\n",
    "    plt.show()\n",
    "\n",
    "# Count class distribution and plot it\n",
    "class_counts = count_class_distribution()\n",
    "plot_class_distribution(class_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1525ab5-9383-4640-bc42-b5f7d70cc069",
   "metadata": {},
   "source": [
    "![classdist](https://i.imgur.com/ClHb4qx.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fc7c180-1061-42a0-9243-75b117f3ff4b",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "997bdf81-fe01-4f65-b382-0c956f5f2a36",
   "metadata": {},
   "source": [
    "# 4. Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce8b49a9-9f1a-4e81-905b-89415be0265e",
   "metadata": {},
   "source": [
    "## 4.1. Splitting the Images into Train, Validation and Test folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37f4a91d-40d8-4115-bb49-5aed3a91dccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = '6k_dataset'\n",
    "images_path = os.path.join(dataset_path, 'images')\n",
    "labels_path = os.path.join(dataset_path, 'labels')\n",
    "\n",
    "# Get all image files\n",
    "image_files = [f for f in os.listdir(images_path) if f.endswith('.jpg')]\n",
    "\n",
    "# Shuffle the files\n",
    "random.shuffle(image_files)\n",
    "\n",
    "# Calculate split indices\n",
    "total_images = len(image_files)\n",
    "train_split = int(total_images * 0.7)\n",
    "val_split = int(total_images * 0.2)\n",
    "\n",
    "# Split the files\n",
    "train_files = image_files[:train_split]\n",
    "val_files = image_files[train_split:train_split + val_split]\n",
    "test_files = image_files[train_split + val_split:]\n",
    "\n",
    "# Copy files to their directories\n",
    "def copy_files(file_list, dest_images_path, dest_labels_path):\n",
    "    os.makedirs(dest_images_path, exist_ok=True)\n",
    "    os.makedirs(dest_labels_path, exist_ok=True)\n",
    "    for file in file_list:\n",
    "        shutil.copy(os.path.join(images_path, file), dest_images_path)\n",
    "        label_file = file.replace('.jpg', '.txt')\n",
    "        if os.path.exists(os.path.join(labels_path, label_file)):\n",
    "            shutil.copy(os.path.join(labels_path, label_file), dest_labels_path)\n",
    "\n",
    "# Create directories and copy files\n",
    "copy_files(train_files, 'yolo_dataset/train/images', 'yolo_dataset/train/labels')\n",
    "copy_files(val_files, 'yolo_dataset/val/images', 'yolo_dataset/val/labels')\n",
    "copy_files(test_files, 'yolo_dataset/test/images', 'yolo_dataset/test/labels')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c18ace9-ea99-45c6-bd2c-108a07841fb2",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6370c78e-1b45-4650-8272-8d134666e31d",
   "metadata": {
    "id": "6370c78e-1b45-4650-8272-8d134666e31d"
   },
   "source": [
    "# 5. Model Selection & Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15b5d01a-2285-446b-b3bf-642d7d9550ae",
   "metadata": {},
   "source": [
    "The YOLOv8m model was selected for its balance of speed and accuracy, crucial for real-time rocket detection. Its pretrained weights accelerate  training, and its robustness to variations ensures reliable detection. The user-friendly API and strong community support further enhance its suitability for this project. Other models were tested first, namely yolov8s and yolov8s pretrained, but yolov8m pretrained yielded the best results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ognfrSmiArG",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "5ognfrSmiArG",
    "outputId": "90b4c25b-fdbf-4240-a4af-d70769d3516d",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = YOLO('yolov8m.pt')\n",
    "\n",
    "training_params = {\n",
    "    'data': dataset_path,  \n",
    "    'imgsz': 640, \n",
    "    'epochs': 100, \n",
    "    'augment': True,  \n",
    "    'patience': 10,  # Early Stopping\n",
    "    'save_period': 1,  \n",
    "    'save': True,  \n",
    "    'resume': True,  \n",
    "    'project': save_dir,\n",
    "    'name': 'my_experiment'\n",
    "}\n",
    "\n",
    "model.train(**training_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "909f6708-8eeb-4df5-89b5-c0ad44bff098",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "yXk_xkB-VrS2",
   "metadata": {
    "id": "yXk_xkB-VrS2"
   },
   "source": [
    "# 6. Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98a30c98-99bf-40c5-bbc8-478019be2bab",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = YOLO(model_path)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "results = model.val(data=dataset_path, save_dir=save_dir)\n",
    "\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1d51c47-bbc9-4041-8433-f7d6cae7ff07",
   "metadata": {},
   "source": [
    "## 6.1. Evaluation Metrics:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "598633f0-95e7-4b94-9dc3-399986c7d0a6",
   "metadata": {},
   "source": [
    "```\n",
    "                 Class     Images  Instances      Precision  Recall     mAP50       mAP50-95)\n",
    "                   all       1221       1390      0.835      0.792      0.826       0.46\n",
    "         Engine Flames        562        570      0.902      0.911      0.945      0.603\n",
    "           Rocket Body        622        633      0.914      0.889      0.925       0.55\n",
    "                 Space        165        187      0.689      0.578      0.608      0.228\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fff52e79-f0e9-46ad-83e8-02ddb9031fa3",
   "metadata": {},
   "source": [
    "- **Precision (P)**: The ratio of correctly predicted positive observations to the total predicted positives. High precision indicates a low false positive rate.\n",
    "- **Recall (R)**: The ratio of correctly predicted positive observations to all observations in the actual class. High recall indicates a low false negative rate.\n",
    "- **mAP50**: Mean Average Precision at IoU threshold of 0.50. It is the average precision across all classes.\n",
    "- **mAP50-95**: Mean Average Precision averaged over IoU thresholds from 0.50 to 0.95. It provides a more comprehensive evaluation of the model's performance.\n",
    "\n",
    "### Analysis\n",
    "- **Overall Performance**: The model achieved an overall precision of **0.835** and recall of **0.792**, with an mAP50 of **0.826** and mAP50-95 of **0.46**. This indicates good overall performance, with a balanced ability to correctly identify and localize objects."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc1208bd-bbf1-4128-88bd-19dcdde7fd7b",
   "metadata": {},
   "source": [
    "## 6.2. Speed Metrics:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef2a6c83-207f-4e8b-ad5d-9b7b370cda16",
   "metadata": {},
   "source": [
    "```\n",
    "Speed: 0.2ms preprocess, 11.4ms inference, 0.0ms loss, 2.2ms postprocess per image\n",
    "Total inference time per image: 13.8ms\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d9e112b-7038-4534-aa18-ff2e199a5b98",
   "metadata": {},
   "source": [
    "This speed is suitable for real-time applications.\n",
    "\n",
    "For a video running at 30 frames per second (FPS), each frame needs to be processed within approximately 33.3ms (1000ms/30). For a 60 FPS video, each frame needs to be processed within approximately 16.7ms (1000ms/60). The model's total processing time of **13.8ms** per image comfortably fits within these time constraints, making it capable of handling both 30 FPS and 60 FPS video streams without causing delays or frame drops."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2873ba4e-7ead-44c6-a569-8b6cb8ee82e1",
   "metadata": {},
   "source": [
    "## 6.3. Confusion Matrix (Normalized)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce5e61bc-c770-474c-99ae-f5ab838e3e9b",
   "metadata": {},
   "source": [
    "![confusionmatrix](https://i.imgur.com/BiFnMuW.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ab43609-5483-4c36-834a-88f0332dda34",
   "metadata": {},
   "source": [
    "The normalized confusion matrix reveals high true positive rates for 'Engine Flames' **(0.92)** and 'Rocket Body' **(0.91)**, indicating strong detection performance for these classes. However, the 'Space' class had a lower true positive rate **(0.64)** and higher misclassification rates. This suggests difficulty in distinguishing the tiny spec of the rocket in space from other classes, likely due to its small size and less distinct features, as well as the class imbalance present in the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5de90dc-3fa1-4161-bd63-f59061abbf08",
   "metadata": {},
   "source": [
    "## 6.4. F1-Confidence Curve"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6731dc4-1945-4e4d-89e7-c34434cb811d",
   "metadata": {},
   "source": [
    "![f1confidence](https://i.imgur.com/3wyVRR7.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09afd484-c1f2-4a05-a281-a4bfe349aa81",
   "metadata": {},
   "source": [
    "The F1-confidence curve shows a peak F1 score of **0.81** at a confidence threshold of **0.312**. This bell-shaped curve indicates that at this confidence level, the model achieves the best balance between precision and recall across all classes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3baa1538-ef8a-4980-bd3d-a5906113a2e9",
   "metadata": {},
   "source": [
    "## 6.5. Visualizing Random Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "otjdnhbnYIJu",
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "otjdnhbnYIJu",
    "outputId": "73b0be37-f0fa-4600-8b84-a68172c1b7af"
   },
   "outputs": [],
   "source": [
    "for result in results:\n",
    "    annotated_image = result.plot()  # Get the annotated image\n",
    "    plt.imshow(annotated_image)\n",
    "    plt.axis('off') \n",
    "    plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ad665880-996f-4009-9dba-675ec7661568",
   "metadata": {},
   "source": [
    "![results](https://i.imgur.com/0KZGDvh.png)\n",
    "![results](https://i.imgur.com/vybRNDm.png)\n",
    "![results](https://i.imgur.com/uQvfFRw.png)\n",
    "![results](https://i.imgur.com/JLeVxiY.png)\n",
    "![results](https://i.imgur.com/IGEDHuU.png)\n",
    "![results](https://i.imgur.com/hTYIWZT.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "423ee950-ce3c-42b8-956c-c12e9276d932",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "JHDMr68jcZVM",
   "metadata": {
    "id": "JHDMr68jcZVM"
   },
   "source": [
    "# 7. Deployment Readiness"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "_UUwdIgbcaa1",
   "metadata": {
    "id": "_UUwdIgbcaa1"
   },
   "source": [
    "The trained YOLOv8m model has been successfully tested on a real-world video of a ULA Atlas V launch that was released few days before completing the training process, demonstrating the model's capability to accurately detect and annotate rocket objects in dynamic environments. This real-world application underscores the model's readiness for deployment in scenarios requiring precise and real-time object detection, such as space mission monitoring and rocket launch analysis.\n",
    "\n",
    "**Video Title**: ULA Atlas V launches final national security mission from Floridaâ€™s Space Coast\n",
    "\n",
    "Channel: WKMG News 6 ClickOrlando\n",
    "\n",
    "**Source**: https://www.youtube.com/watch?v=GYWr3FV9umU\n",
    "\n",
    "**Link of the processed video**: https://www.youtube.com/watch?v=iVRCdFLpDaE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "qBxDx5G7IKos",
   "metadata": {
    "id": "qBxDx5G7IKos"
   },
   "outputs": [],
   "source": [
    "model = YOLO(model_path)\n",
    "\n",
    "video_path = '/content/drive/My Drive/yolo_dataset/launch_video.mp4'\n",
    "\n",
    "# Directory to save the annotated video\n",
    "experiment_name = 'video_inference'\n",
    "\n",
    "# Run inference on the video with stream=True (to avoid memory crashes)\n",
    "results = model.predict(source=video_path, stream=True, save=True, project=save_dir, name=experiment_name)\n",
    "\n",
    "# Process the results\n",
    "for r in results:\n",
    "    boxes = r.boxes  # Boxes object for bbox outputs\n",
    "    masks = r.masks  # Masks object for segment masks outputs\n",
    "    probs = r.probs  # Class probabilities for classification outputs\n",
    "\n",
    "print(f\"Annotated video saved to {save_dir}/{experiment_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33eaae42-40ec-405e-bbd4-3441e664fd79",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5d4d998-93d6-45e0-90b6-a1d57451fc32",
   "metadata": {},
   "source": [
    "# 8. Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b15a0d6-7fb0-40bd-9028-64a6e378be8f",
   "metadata": {},
   "source": [
    "Our project successfully developed a robust YOLOv8m-based model that accurately identifies and localizes rocket components such as engine flames, rocket bodies, and space observation in real-time. The model achieved an overall precision of **0.835** and recall of **0.792**, with an mAP50 of **0.826** and mAP50-95 of **0.46**, demonstrating its effectiveness in dynamic environments. The system's ability to process images at **13.8ms** per frame makes it suitable for real-time applications, enhancing efficiency in rocket launch monitoring. **Further recommendations** include expanding the dataset to improve class balance, training the model for longer than 100 epochs, performing hyperparamter optimization,  integrating the model with real-time monitoring systems, and exploring advanced techniques like multi-scale training to enhance detection accuracy for smaller objects."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
